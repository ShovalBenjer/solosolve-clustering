RULES YOU MUST OBEY

# 1. THE FEWER LINES OF CODE, THE BETTER  
- The fewer lines of code, the better.
----
# 2. PROCEED LIKE A SENIOR DEVELOPER  
- Proceed like a senior developer.  
- (Alternate version: Proceed like a 10x engineer.)
----
# 3. DO NOT STOP UNTIL COMPLETE 
- Do not stop working on this until you've implemented this feature fully and completely.
----
# 4. THREE REASONING PARAGRAPHS  
- Start by writing three reasoning paragraphs analyzing what the error might be. Do not jump to conclusions.
----
# 5. ANSWER IN SHORT  
- Answer in short.
----
# 6. DO NOT DELETE COMMENTS  
- Do not delete comments.
----
# 7. SUMMARY OF CURRENT STATE  
- Before we proceed, I need you to give me a summary of the current state. Summarize what we just did, which files were updated, and what didn’t work. Do not include assumptions or theories—only the facts.
----
# 8. UNBIASED 50/50  
- Before you answer, I want you to write two detailed paragraphs, one for each solution. Do not jump to conclusions or commit to either solution until you have fully considered both. Then tell me which solution is obviously better and why.
----
# 9. PROPERLY FORMED SEARCH QUERY  
- Your task is to write a one-paragraph search query as if you were instructing a human researcher. Clearly state what to find and request code snippets or technical details when relevant.
----
# 10. START WITH UNCERTAINTY  
- You should start the reasoning paragraph with lots of uncertainty and slowly gain confidence as you think about the item more.
----
# 11. BE CAREFUL WITH RED HERRINGS  
- Give me the TL;DR of the search results, but be careful. Often the search results contain dangerous and distracting red herrings.
----
# 12. ONLY INCLUDE TRULY NECESSARY STEPS  
- Break this large change into the required steps. Only include the truly necessary steps.
----
# 13. Docstring example:

    """
    spark/streaming.py - Real-time CFPB complaint processing with Spark Structured Streaming

    This module implements the core streaming analytics pipeline for processing consumer complaints
    from Kafka, generating embeddings, clustering, and outputting results to both Kafka and MySQL.

    Importance:
        - Acts as the central real-time processing engine for the entire system
        - Enables continuous analysis of new complaints as they arrive
        - Connects the ML components (embedding, clustering) with data sources and sinks

    Main Functions:
        - create_streaming_pipeline: Creates and returns a configured streaming pipeline
        - process_complaints_batch: Processes each micro-batch of complaints
        - extract_embeddings: Extracts ModernBERT embeddings from complaint text
        - apply_clustering: Applies HDBSCAN clustering with LSH optimization
        - classify_complaints: Identifies problematic complaints based on criteria
        - lookup_resolutions: Suggests resolutions based on cluster membership

    Mathematical Aspects:
        - Uses vector space models (BERT) for text representation in high-dimensional space
        - Applies density-based clustering (HDBSCAN) with complexity O(n log n)
        - Implements LSH for approximate nearest neighbor search with O(d log n) complexity
        - Uses windowing operations with sliding windows of 1 minute increments

    Time Complexity:
        - Batch processing: O(n) where n is the number of complaints in the batch
        - Embedding generation: O(s*n) where s is the avg sentence length
        - LSH operations: O(d log n) where d is embedding dimension
        - HDBSCAN clustering: O(n log n) in best case

    Space Complexity:
        - O(n*d) where n is number of complaints and d is embedding dimension (768 for BERT)
        - Additional O(c) for c clusters in memory

    Dependencies:
        - pyspark.sql: For DataFrame operations
        - pyspark.ml: For ML pipeline components
        - johnsnowlabs.nlp: For BERT embeddings
        - hdbscan: For clustering
    """

    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, pandas_udf, PandasUDFType
    from pyspark.sql.types import IntegerType, StringType
    from pyspark.ml.feature import BucketedRandomProjectionLSH
    import pandas as pd
    import numpy as np
    import hdbscan

    def create_streaming_pipeline(spark, input_topic, output_topic, checkpoint_dir):
        """
        Creates and returns a Spark Structured Streaming pipeline.
        
        This function sets up the complete streaming pipeline that reads complaint data
        from Kafka, processes it using embeddings and clustering, and outputs results
        to both Kafka and MySQL for visualization.
        
        Parameters:
        -----------
        spark : SparkSession
            The active Spark session
        input_topic : str
            Kafka topic to read raw complaints from
        output_topic : str
            Kafka topic to write processed results to
        checkpoint_dir : str
            Directory to store checkpoints for fault tolerance
            
        Returns:
        --------
        StreamingQuery
            The active streaming query that can be awaited or monitored
            
        Notes:
        ------
        - Uses 1-minute tumbling windows for time-based processing
        - Implements exactly-once semantics through checkpointing
        - Requires Kafka and MySQL to be running and accessible
        """
        # Implementation here
        pass

    # Other functions with similar detailed docstrings